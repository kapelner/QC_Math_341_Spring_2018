\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 341 / 650.3 Spring 2018 Homework \#6}

\author{Professor Adam Kapelner} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due Friday 11:59PM, May 18, 2018 under the door of KY604\\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, read about the normal-normal semi-conjugate model, grid sampling, Gibbs sampling, the Metropolis-Hastings algorthim and read the relevant sections of McGrayne. 

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

Problems marked \qu{[MA]} are for the masters students only (those enrolled in the 650.3 course). For those in 341, doing these questions will count as extra credit.

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}


\problem{These are questions about McGrayne's book, chapter 17 and the Epilogue.}

\begin{enumerate}

\easysubproblem{[optional] What do the computer scientists who adopted Bayesian methods care most about and whose view do they subscribe to? (p233)}\spc{1}

\easysubproblem{[optional] How was \qu{Stanley} able to cross the Nevada desert?}\spc{3}

\easysubproblem{[optional] What two factors are leading to the \qu{crumbling of the Tower of Babel?}}\spc{3}

\intermediatesubproblem{[optional] Does the brain work through iterative Bayesian modeling?}\spc{4}

\easysubproblem{[optional] According to Geman, what is the most powerful argument for Bayesian Statistics?}\spc{3}

\end{enumerate}

\input{R_functions_table}

\problem{This problem is about the normal-normal model using a \qu{semi-conjugate} prior. Assume $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ throughout.}

\begin{enumerate}

\easysubproblem{If $\theta$ and $\sigsq$ are assumed to be independent, how can $\prob{\theta,~\sigsq}$ be factored?}\spc{1}

\easysubproblem{If $\theta \sim \normnot{\mu_0}{\tausq}$ and $\sigsq \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$, find the kernel of $\prob{\theta,~\sigsq}$.}\spc{2}

\easysubproblem{Using your answer to (b), find the kernel of $\cprob{\theta,~\sigsq}{X}$}.\spc{4}

\hardsubproblem{Show that the kernel in (c) cannot be factored into the kernel of a normal and the kernel of an inverse gamma. This is in the lecture.}\spc{9}

\hardsubproblem{Your answer to (d) looks like a normal and a $k(\sigsq~|~X)$. Find the posterior mode of $\theta$.}\spc{10}

\hardsubproblem{Describe how you would sample from $\cprob{\theta,~\sigsq}{X}$. Make all steps explicit and use the notation from Table~\ref{tab:eqs}.}\spc{10}


\hardsubproblem{What are the disadvantages of grid sampling?}\spc{6}

\hardsubproblem{What's the bad part about not using a conjugate model?}\spc{4}


\extracreditsubproblem{[MA] Find the MMSE of $\sigsq$}\spc{6}

\end{enumerate}

\problem{These are questions which introduce Gibbs Sampling.}

\begin{enumerate}
\easysubproblem{Outline the systematic sweep Gibbs Sampler algorithm below (in your notes).}\spc{8}

\extracreditsubproblem{Under what conditions does this algorithm converge?}\spc{8}

\easysubproblem{Pretend you are estimating $\cprob{\theta_1,~\theta_2}{X}$ and the joint posterior looks like the picture below where the $x$ axis is $\theta_1$ and the $y$ axis is $\theta_2$ and darker colors indicate higher probability. Begin at $\bracks{\theta_1,\theta_2} = \bracks{0.5,0.5}$ and simulate 5 iterations of the systematic sweep Gibbs sampling algorithm by drawing new points on the plot (just as we did in class).}


\begin{figure}[htp]
\centering
\includegraphics[width=4in]{contour.png}
\end{figure}
\end{enumerate}


\problem{These are questions about the change point model and the Gibbs sampler to draw inference for its parameters. You will have to use R to do this question. If you do not have it installed on your computer, you can use R online without installing anything by using \href{http://www.r-fiddle.org}{R-Fiddle}. After plots pop up you can make the plots bigger by resizing.}

\begin{enumerate}

\easysubproblem{Consider the change point Poisson model we looked at in class. We have $m$ exchangeable Poisson r.v.'s with parameter $\lambda_1$ followed by $n-m$ exchangeable Poisson r.v.'s with parameter $\lambda_2$. Both rate parameters and the value of $m$ are unknown so the parameter space is 3-dimensional. Write the likelihood below.}\spc{4}

\easysubproblem{Consider the model in (a) where $\lambda_1 = 2$ and $\lambda_2 = 4$ and $m=10$ and $n=30$. Run the code on lines 1--14 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here} by copying them from the website and pasting them into an R console. This will plot a realization of the data with those parameters. Can you identify the change point visually?}\spc{1}

\easysubproblem{Consider the model in (a) but we are blinded to the true values of the parameters given in (b) and we wish to estimate them via a Gibbs sampler. Run the code on lines 16--78 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here} which will run 10,000 iterations. What iteration number do you think the sampler converged?}\spc{1}

\easysubproblem{Now we wish to assess autocorrelation among the chains from the Gibbs sampler run in (d). Run the code on lines 79--89 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here}. What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{1}

\easysubproblem{Run the code on lines 91--121 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here} which will first burn and thin the chains. Explain these three plots. What distributions do these frequency histograms approximate? You must have $\prob{\text{something}}$ in your answer. What are the blue lines? What are the red lines? What are the grey lines? Read the code if you have to for the answers.}\spc{4}

\hardsubproblem{Test the following hypothesis: $H_0: m \leq 15$ by approximating the $p$-value from one of the plots in (e).}\spc{4}

\hardsubproblem{[M.A.] Explain a procedure to test $H_0: \lambda_1 = \lambda_2$. You can use the plots if you wish, but you do not have to.}\spc{8}

\hardsubproblem{What exactly would come from $\cprob{X^*}{X}$ in the context of this problem? Assume $X^*$ is the same dimension of $X$ (in our toy example, $n=30$). Explain in full detail. Be careful!}\spc{4}

\extracreditsubproblem{Explain how you would estimate $\cov{\lambda_1}{\lambda_2}$ and what do you think this estimate will be close to?}\spc{7}


\end{enumerate}


\problem{These are questions about the mixture-of-two-normals model and the Gibbs sampler to draw inference for its parameters. You will have to use R to do this question. If you do not have it installed on your computer you can use \href{http://rextester.com/l/r_online_compiler}{this website} which will give you provide you with a workable R console.}

\begin{enumerate}


\easysubproblem{Consider the mixture-of-two-normals model we looked at in class. Write the likelihood below.}\spc{4}

\easysubproblem{Consider the model in (a) with $\theta_1 = 0,~\theta_2 =4,~\sigsq_1 = 2,~\sigsq_2 = 1$ and $\rho = 2$. These are the targets of inference so pretend you don't know their values! Run the code on lines 1--16 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here} by copying them from the website and pasting them into an R console. This will plot a realization of the data with those parameters. Can you identify that it's a mixture of two normals visually?}\spc{1}

\easysubproblem{Consider the model in (a) but we are blinded to the true values of the parameters given in (b) and we wish to estimate them via a Gibbs sampler. Run the code on lines 19--92 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here} which will run 10,000 iterations. What iteration number do you think the sampler converged?}\spc{1}

\easysubproblem{Now we wish to assess autocorrelation among the chains from the Gibbs sampler run in (d). Run the code on lines 96--103 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here}. What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{1}

\easysubproblem{Run the code on lines 120--152 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here} which will first burn and thin the chains. Explain these five plots. What distributions do these frequency histograms approximate?}\spc{4}

\hardsubproblem{Provide and approximate $CR_{\rho, 95\%}$. Does it capture the true value of $\rho$?}\spc{4}

\hardsubproblem{Explain carefully how you would approximate $\cprob{X^*}{X}$.}\spc{4}


\hardsubproblem{Explain carefully how you would approximate the probability that the 17th observation belonged to the $\normnot{\theta_1}{\sigsq_1}$ distribution.}\spc{4}

\easysubproblem{If one of the $\theta$'s did not have a known conditional distribution, which algorithm could you use? Would this algorithm take longer or shorter to converge than the Gibbs sampler you've seen here?}\spc{3}


\extracreditsubproblem{Explain carefully how you would test if $\theta_1 \neq \theta_2$.}\spc{7}

\end{enumerate}


\end{document}

